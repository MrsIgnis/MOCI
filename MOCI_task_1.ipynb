{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMk+b7RslYgl8gJNypvSS0s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrsIgnis/MOCI/blob/main/MOCI_task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "ILiUyuIzkzyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c549915f-6b92-49d3-a91c-3adc1a122707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pymorphy3\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "14hVZlhkuO9w"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zEssSou8QNE",
        "outputId": "f229d273-a394-46fb-ed03-35d04e70df71"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_ru = set(stopwords.words(\"russian\"))\n",
        "stop_words_en = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "2gWBKQQvM5Yi"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/test.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "4wDv_TmB4Q5q"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer_ru = SnowballStemmer(\"russian\")\n",
        "stemmer_en = SnowballStemmer(\"english\")\n",
        "lemma_ru = pymorphy3.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "1qU-ix758f7V"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text: str) -> tuple[list[str], list[str]]:\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "\n",
        "    lemmas = [lemma_ru.parse(word)[0].normal_form for word in words if word not in stop_words_ru]\n",
        "    stems = [stemmer_ru.stem(word) for word in words if word not in stop_words_ru]\n",
        "\n",
        "    return lemmas, stems"
      ],
      "metadata": {
        "id": "Z9Vtq-G7-Tyk"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_ascii(text: str) -> list[str]:\n",
        "    return [char for char in text if ord(char) <= 255]"
      ],
      "metadata": {
        "id": "sB8h7F9nJfx3"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_ascii(text: list[str]) -> list[int]:\n",
        "    return [ord(char) for char in text if ord(char) <= 255]"
      ],
      "metadata": {
        "id": "SaK6Q569X_Eh"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas, stems = preprocess_text(text)\n",
        "processed_text = ' '.join(lemmas + stems)\n",
        "ascii_tokens = tokenize_ascii(processed_text)\n",
        "ascii_vectors = vectorize_ascii(processed_text)"
      ],
      "metadata": {
        "id": "PUtHm016-lnv"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Лемматизированный текст:\", lemmas, '\\n')\n",
        "print(\"Стеммированный текст:\", stems, '\\n')\n",
        "print(\"Токенизированный текст:\", ascii_tokens, '\\n')\n",
        "print(\"Векторизированный текст:\", ascii_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBYWMJ8mgs47",
        "outputId": "a543d6e5-5726-4fed-c090-c1f116ee3f65"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированный текст: ['скромняга', 'бард', 'отдыхать', 'дело', 'геральт', 'ривия', 'песня', 'петь', 'when', 'a', 'humble', 'bard', 'graced', 'a', 'ride', 'along', 'with', 'geralt', 'of', 'rivia', 'along', 'came', 'this', 'song'] \n",
            "\n",
            "Стеммированный текст: ['скромняг', 'бард', 'отдыха', 'дел', 'геральт', 'рив', 'песн', 'пел', 'when', 'a', 'humble', 'bard', 'graced', 'a', 'ride', 'along', 'with', 'geralt', 'of', 'rivia', 'along', 'came', 'this', 'song'] \n",
            "\n",
            "Токенизированный текст: [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'w', 'h', 'e', 'n', ' ', 'a', ' ', 'h', 'u', 'm', 'b', 'l', 'e', ' ', 'b', 'a', 'r', 'd', ' ', 'g', 'r', 'a', 'c', 'e', 'd', ' ', 'a', ' ', 'r', 'i', 'd', 'e', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'g', 'e', 'r', 'a', 'l', 't', ' ', 'o', 'f', ' ', 'r', 'i', 'v', 'i', 'a', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 'c', 'a', 'm', 'e', ' ', 't', 'h', 'i', 's', ' ', 's', 'o', 'n', 'g', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'w', 'h', 'e', 'n', ' ', 'a', ' ', 'h', 'u', 'm', 'b', 'l', 'e', ' ', 'b', 'a', 'r', 'd', ' ', 'g', 'r', 'a', 'c', 'e', 'd', ' ', 'a', ' ', 'r', 'i', 'd', 'e', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'g', 'e', 'r', 'a', 'l', 't', ' ', 'o', 'f', ' ', 'r', 'i', 'v', 'i', 'a', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 'c', 'a', 'm', 'e', ' ', 't', 'h', 'i', 's', ' ', 's', 'o', 'n', 'g'] \n",
            "\n",
            "Векторизированный текст: [32, 32, 32, 32, 32, 32, 32, 32, 119, 104, 101, 110, 32, 97, 32, 104, 117, 109, 98, 108, 101, 32, 98, 97, 114, 100, 32, 103, 114, 97, 99, 101, 100, 32, 97, 32, 114, 105, 100, 101, 32, 97, 108, 111, 110, 103, 32, 119, 105, 116, 104, 32, 103, 101, 114, 97, 108, 116, 32, 111, 102, 32, 114, 105, 118, 105, 97, 32, 97, 108, 111, 110, 103, 32, 99, 97, 109, 101, 32, 116, 104, 105, 115, 32, 115, 111, 110, 103, 32, 32, 32, 32, 32, 32, 32, 32, 32, 119, 104, 101, 110, 32, 97, 32, 104, 117, 109, 98, 108, 101, 32, 98, 97, 114, 100, 32, 103, 114, 97, 99, 101, 100, 32, 97, 32, 114, 105, 100, 101, 32, 97, 108, 111, 110, 103, 32, 119, 105, 116, 104, 32, 103, 101, 114, 97, 108, 116, 32, 111, 102, 32, 114, 105, 118, 105, 97, 32, 97, 108, 111, 110, 103, 32, 99, 97, 109, 101, 32, 116, 104, 105, 115, 32, 115, 111, 110, 103]\n"
          ]
        }
      ]
    }
  ]
}