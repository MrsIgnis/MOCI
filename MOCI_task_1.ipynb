{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOH7aqU3JYTht2KdHkl4axY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrsIgnis/MOCI/blob/main/MOCI_task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ILiUyuIzkzyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d22356-331e-4b60-bbca-8dc44deb8a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pymorphy3\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "14hVZlhkuO9w"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zEssSou8QNE",
        "outputId": "94697cc3-38a4-4cc4-d31f-b7a901a75cad"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_ru = set(stopwords.words(\"russian\"))\n",
        "stop_words_en = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "2gWBKQQvM5Yi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer_ru = SnowballStemmer(\"russian\")\n",
        "stemmer_en = SnowballStemmer(\"english\")\n",
        "lemma_ru = pymorphy3.MorphAnalyzer()\n",
        "lemma_en = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "1qU-ix758f7V"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/test.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02TuNnSvQ2OC",
        "outputId": "d4b00d88-461a-4a3f-fef1-b8e2abf7e8c2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When a humble bard\n",
            "Graced a ride along\n",
            "With Geralt of Rivia\n",
            "Along came this song.\n",
            "From when the White Wolf fought\n",
            "A silver-tongued devil\n",
            "His army of elves...\n",
            "\n",
            "Когда скромняга бард отдыхал от дел\n",
            "С Геральтом из Ривии он песню эту пел.\n",
            "Сразился Белый Волк с велиречивым чертом\n",
            "Эльфов покромсал несчетные когорты.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I. Провести на любом тексте лемматизацию и стемминг (nltk, pymorphy2, pymorphy3, natasha)**"
      ],
      "metadata": {
        "id": "Oi4BGuHZS4yT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_russian(word: str) -> bool:\n",
        "    return bool(re.search('[а-яё]', word, re.IGNORECASE))\n",
        "\n",
        "def is_english(word: str) -> bool:\n",
        "    return bool(re.search('[a-z]', word, re.IGNORECASE))"
      ],
      "metadata": {
        "id": "hEMe-C0vP302"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^а-яёa-z\\s]', '', text, flags=re.IGNORECASE)\n",
        "    words = word_tokenize(text)\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        if word not in stop_words_ru and word not in stop_words_en:\n",
        "            if is_russian(word):\n",
        "                lemma = lemma_ru.parse(word)[0].normal_form\n",
        "            elif is_english(word):\n",
        "                lemma = lemma_en.lemmatize(word)\n",
        "            else:\n",
        "                lemma = word\n",
        "            lemmas.append(lemma)\n",
        "    return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "Z9Vtq-G7-Tyk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^а-яёa-z\\s]', '', text, flags=re.IGNORECASE)\n",
        "    words = word_tokenize(text)\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        if word not in stop_words_ru and word not in stop_words_en:\n",
        "            if is_russian(word):\n",
        "                stem = stemmer_ru.stem(word)\n",
        "            elif is_english(word):\n",
        "                stem = stemmer_en.stem(word)\n",
        "            else:\n",
        "                stem = word\n",
        "            stems.append(stem)\n",
        "    return ' '.join(stems)"
      ],
      "metadata": {
        "id": "93W4PSArwET1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas_text = lemmatize_text(text)\n",
        "stems_text = stem_text(text)\n",
        "print(\"Лемматизированный исходный текст:\", lemmas_text, '\\n')\n",
        "print(\"Стеммированный исходный текст:\", stems_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkYbkwHY32aa",
        "outputId": "28fd7e2a-3da1-4c6e-f5d6-af0d1a929377"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированный исходный текст: humble bard graced ride along geralt rivia along came song white wolf fought silvertongued devil army elf скромняга бард отдыхать дело геральт ривия песня петь сразиться белый волк велиречивый чёрт эльф покромсать несчётный когорта \n",
            "\n",
            "Стеммированный исходный текст: humbl bard grace ride along geralt rivia along came song white wolf fought silvertongu devil armi elv скромняг бард отдыха дел геральт рив песн пел сраз бел волк велиречив черт эльф покромса несчетн когорт\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Написать функцию для токенизации всех символов из ASCII**"
      ],
      "metadata": {
        "id": "SqVLYWF0wrhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_ascii(text: str) -> list[str]:\n",
        "    return [char for char in text if not char.isspace()]"
      ],
      "metadata": {
        "id": "sB8h7F9nJfx3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ascii_tokens = tokenize_ascii(text)\n",
        "print(\"Токенизированный исходный текст:\", ascii_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUW0hhClw3-w",
        "outputId": "9c73d194-2472-47d3-f5c9-792b371986c4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизированный исходный текст: ['W', 'h', 'e', 'n', 'a', 'h', 'u', 'm', 'b', 'l', 'e', 'b', 'a', 'r', 'd', 'G', 'r', 'a', 'c', 'e', 'd', 'a', 'r', 'i', 'd', 'e', 'a', 'l', 'o', 'n', 'g', 'W', 'i', 't', 'h', 'G', 'e', 'r', 'a', 'l', 't', 'o', 'f', 'R', 'i', 'v', 'i', 'a', 'A', 'l', 'o', 'n', 'g', 'c', 'a', 'm', 'e', 't', 'h', 'i', 's', 's', 'o', 'n', 'g', '.', 'F', 'r', 'o', 'm', 'w', 'h', 'e', 'n', 't', 'h', 'e', 'W', 'h', 'i', 't', 'e', 'W', 'o', 'l', 'f', 'f', 'o', 'u', 'g', 'h', 't', 'A', 's', 'i', 'l', 'v', 'e', 'r', '-', 't', 'o', 'n', 'g', 'u', 'e', 'd', 'd', 'e', 'v', 'i', 'l', 'H', 'i', 's', 'a', 'r', 'm', 'y', 'o', 'f', 'e', 'l', 'v', 'e', 's', '.', '.', '.', 'К', 'о', 'г', 'д', 'а', 'с', 'к', 'р', 'о', 'м', 'н', 'я', 'г', 'а', 'б', 'а', 'р', 'д', 'о', 'т', 'д', 'ы', 'х', 'а', 'л', 'о', 'т', 'д', 'е', 'л', 'С', 'Г', 'е', 'р', 'а', 'л', 'ь', 'т', 'о', 'м', 'и', 'з', 'Р', 'и', 'в', 'и', 'и', 'о', 'н', 'п', 'е', 'с', 'н', 'ю', 'э', 'т', 'у', 'п', 'е', 'л', '.', 'С', 'р', 'а', 'з', 'и', 'л', 'с', 'я', 'Б', 'е', 'л', 'ы', 'й', 'В', 'о', 'л', 'к', 'с', 'в', 'е', 'л', 'и', 'р', 'е', 'ч', 'и', 'в', 'ы', 'м', 'ч', 'е', 'р', 'т', 'о', 'м', 'Э', 'л', 'ь', 'ф', 'о', 'в', 'п', 'о', 'к', 'р', 'о', 'м', 'с', 'а', 'л', 'н', 'е', 'с', 'ч', 'е', 'т', 'н', 'ы', 'е', 'к', 'о', 'г', 'о', 'р', 'т', 'ы', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III. Написать функцию для векторизации всех символов из ASCII**"
      ],
      "metadata": {
        "id": "tXkTVNsI1RqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_ascii(text: list[str]) -> list[int]:\n",
        "    return [ord(char) for char in text if not char.isspace()]"
      ],
      "metadata": {
        "id": "SaK6Q569X_Eh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ascii_vectors = vectorize_ascii(text)\n",
        "print(\"Векторизированный исходный текст:\", ascii_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxG5ksm11YRg",
        "outputId": "578e8f41-7c83-44f5-ddad-3393dc70b14e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Векторизированный исходный текст: [87, 104, 101, 110, 97, 104, 117, 109, 98, 108, 101, 98, 97, 114, 100, 71, 114, 97, 99, 101, 100, 97, 114, 105, 100, 101, 97, 108, 111, 110, 103, 87, 105, 116, 104, 71, 101, 114, 97, 108, 116, 111, 102, 82, 105, 118, 105, 97, 65, 108, 111, 110, 103, 99, 97, 109, 101, 116, 104, 105, 115, 115, 111, 110, 103, 46, 70, 114, 111, 109, 119, 104, 101, 110, 116, 104, 101, 87, 104, 105, 116, 101, 87, 111, 108, 102, 102, 111, 117, 103, 104, 116, 65, 115, 105, 108, 118, 101, 114, 45, 116, 111, 110, 103, 117, 101, 100, 100, 101, 118, 105, 108, 72, 105, 115, 97, 114, 109, 121, 111, 102, 101, 108, 118, 101, 115, 46, 46, 46, 1050, 1086, 1075, 1076, 1072, 1089, 1082, 1088, 1086, 1084, 1085, 1103, 1075, 1072, 1073, 1072, 1088, 1076, 1086, 1090, 1076, 1099, 1093, 1072, 1083, 1086, 1090, 1076, 1077, 1083, 1057, 1043, 1077, 1088, 1072, 1083, 1100, 1090, 1086, 1084, 1080, 1079, 1056, 1080, 1074, 1080, 1080, 1086, 1085, 1087, 1077, 1089, 1085, 1102, 1101, 1090, 1091, 1087, 1077, 1083, 46, 1057, 1088, 1072, 1079, 1080, 1083, 1089, 1103, 1041, 1077, 1083, 1099, 1081, 1042, 1086, 1083, 1082, 1089, 1074, 1077, 1083, 1080, 1088, 1077, 1095, 1080, 1074, 1099, 1084, 1095, 1077, 1088, 1090, 1086, 1084, 1069, 1083, 1100, 1092, 1086, 1074, 1087, 1086, 1082, 1088, 1086, 1084, 1089, 1072, 1083, 1085, 1077, 1089, 1095, 1077, 1090, 1085, 1099, 1077, 1082, 1086, 1075, 1086, 1088, 1090, 1099, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IV. Провести токенизацию и векторизацию текста после лемматизации и стемминга**"
      ],
      "metadata": {
        "id": "sqNdsIbq1kqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas_ascii_tokens = tokenize_ascii(lemmas_text)\n",
        "lemmas_ascii_vectors = vectorize_ascii(lemmas_ascii_tokens)\n",
        "print(\"Токенизация лемм:\", lemmas_ascii_tokens, '\\n')\n",
        "print(\"Векторизация лемм:\", lemmas_ascii_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQoOaTww11Rs",
        "outputId": "27c8c0d7-a254-4f00-ebf9-28adc12c934b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация лемм: ['h', 'u', 'm', 'b', 'l', 'e', 'b', 'a', 'r', 'd', 'g', 'r', 'a', 'c', 'e', 'd', 'r', 'i', 'd', 'e', 'a', 'l', 'o', 'n', 'g', 'g', 'e', 'r', 'a', 'l', 't', 'r', 'i', 'v', 'i', 'a', 'a', 'l', 'o', 'n', 'g', 'c', 'a', 'm', 'e', 's', 'o', 'n', 'g', 'w', 'h', 'i', 't', 'e', 'w', 'o', 'l', 'f', 'f', 'o', 'u', 'g', 'h', 't', 's', 'i', 'l', 'v', 'e', 'r', 't', 'o', 'n', 'g', 'u', 'e', 'd', 'd', 'e', 'v', 'i', 'l', 'a', 'r', 'm', 'y', 'e', 'l', 'f', 'с', 'к', 'р', 'о', 'м', 'н', 'я', 'г', 'а', 'б', 'а', 'р', 'д', 'о', 'т', 'д', 'ы', 'х', 'а', 'т', 'ь', 'д', 'е', 'л', 'о', 'г', 'е', 'р', 'а', 'л', 'ь', 'т', 'р', 'и', 'в', 'и', 'я', 'п', 'е', 'с', 'н', 'я', 'п', 'е', 'т', 'ь', 'с', 'р', 'а', 'з', 'и', 'т', 'ь', 'с', 'я', 'б', 'е', 'л', 'ы', 'й', 'в', 'о', 'л', 'к', 'в', 'е', 'л', 'и', 'р', 'е', 'ч', 'и', 'в', 'ы', 'й', 'ч', 'ё', 'р', 'т', 'э', 'л', 'ь', 'ф', 'п', 'о', 'к', 'р', 'о', 'м', 'с', 'а', 'т', 'ь', 'н', 'е', 'с', 'ч', 'ё', 'т', 'н', 'ы', 'й', 'к', 'о', 'г', 'о', 'р', 'т', 'а'] \n",
            "\n",
            "Векторизация лемм: [104, 117, 109, 98, 108, 101, 98, 97, 114, 100, 103, 114, 97, 99, 101, 100, 114, 105, 100, 101, 97, 108, 111, 110, 103, 103, 101, 114, 97, 108, 116, 114, 105, 118, 105, 97, 97, 108, 111, 110, 103, 99, 97, 109, 101, 115, 111, 110, 103, 119, 104, 105, 116, 101, 119, 111, 108, 102, 102, 111, 117, 103, 104, 116, 115, 105, 108, 118, 101, 114, 116, 111, 110, 103, 117, 101, 100, 100, 101, 118, 105, 108, 97, 114, 109, 121, 101, 108, 102, 1089, 1082, 1088, 1086, 1084, 1085, 1103, 1075, 1072, 1073, 1072, 1088, 1076, 1086, 1090, 1076, 1099, 1093, 1072, 1090, 1100, 1076, 1077, 1083, 1086, 1075, 1077, 1088, 1072, 1083, 1100, 1090, 1088, 1080, 1074, 1080, 1103, 1087, 1077, 1089, 1085, 1103, 1087, 1077, 1090, 1100, 1089, 1088, 1072, 1079, 1080, 1090, 1100, 1089, 1103, 1073, 1077, 1083, 1099, 1081, 1074, 1086, 1083, 1082, 1074, 1077, 1083, 1080, 1088, 1077, 1095, 1080, 1074, 1099, 1081, 1095, 1105, 1088, 1090, 1101, 1083, 1100, 1092, 1087, 1086, 1082, 1088, 1086, 1084, 1089, 1072, 1090, 1100, 1085, 1077, 1089, 1095, 1105, 1090, 1085, 1099, 1081, 1082, 1086, 1075, 1086, 1088, 1090, 1072]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stems_ascii_tokens = tokenize_ascii(stems_text)\n",
        "stems_ascii_vectors = vectorize_ascii(stems_ascii_tokens)\n",
        "print(\"Токенизация стеммов:\", stems_ascii_tokens, '\\n')\n",
        "print(\"Векторизация стеммов:\", stems_ascii_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0EAYvau2IlU",
        "outputId": "f0c6b1e4-514a-4d8a-e9e0-9af8fd569e03"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация стеммов: ['h', 'u', 'm', 'b', 'l', 'b', 'a', 'r', 'd', 'g', 'r', 'a', 'c', 'e', 'r', 'i', 'd', 'e', 'a', 'l', 'o', 'n', 'g', 'g', 'e', 'r', 'a', 'l', 't', 'r', 'i', 'v', 'i', 'a', 'a', 'l', 'o', 'n', 'g', 'c', 'a', 'm', 'e', 's', 'o', 'n', 'g', 'w', 'h', 'i', 't', 'e', 'w', 'o', 'l', 'f', 'f', 'o', 'u', 'g', 'h', 't', 's', 'i', 'l', 'v', 'e', 'r', 't', 'o', 'n', 'g', 'u', 'd', 'e', 'v', 'i', 'l', 'a', 'r', 'm', 'i', 'e', 'l', 'v', 'с', 'к', 'р', 'о', 'м', 'н', 'я', 'г', 'б', 'а', 'р', 'д', 'о', 'т', 'д', 'ы', 'х', 'а', 'д', 'е', 'л', 'г', 'е', 'р', 'а', 'л', 'ь', 'т', 'р', 'и', 'в', 'п', 'е', 'с', 'н', 'п', 'е', 'л', 'с', 'р', 'а', 'з', 'б', 'е', 'л', 'в', 'о', 'л', 'к', 'в', 'е', 'л', 'и', 'р', 'е', 'ч', 'и', 'в', 'ч', 'е', 'р', 'т', 'э', 'л', 'ь', 'ф', 'п', 'о', 'к', 'р', 'о', 'м', 'с', 'а', 'н', 'е', 'с', 'ч', 'е', 'т', 'н', 'к', 'о', 'г', 'о', 'р', 'т'] \n",
            "\n",
            "Векторизация стеммов: [104, 117, 109, 98, 108, 98, 97, 114, 100, 103, 114, 97, 99, 101, 114, 105, 100, 101, 97, 108, 111, 110, 103, 103, 101, 114, 97, 108, 116, 114, 105, 118, 105, 97, 97, 108, 111, 110, 103, 99, 97, 109, 101, 115, 111, 110, 103, 119, 104, 105, 116, 101, 119, 111, 108, 102, 102, 111, 117, 103, 104, 116, 115, 105, 108, 118, 101, 114, 116, 111, 110, 103, 117, 100, 101, 118, 105, 108, 97, 114, 109, 105, 101, 108, 118, 1089, 1082, 1088, 1086, 1084, 1085, 1103, 1075, 1073, 1072, 1088, 1076, 1086, 1090, 1076, 1099, 1093, 1072, 1076, 1077, 1083, 1075, 1077, 1088, 1072, 1083, 1100, 1090, 1088, 1080, 1074, 1087, 1077, 1089, 1085, 1087, 1077, 1083, 1089, 1088, 1072, 1079, 1073, 1077, 1083, 1074, 1086, 1083, 1082, 1074, 1077, 1083, 1080, 1088, 1077, 1095, 1080, 1074, 1095, 1077, 1088, 1090, 1101, 1083, 1100, 1092, 1087, 1086, 1082, 1088, 1086, 1084, 1089, 1072, 1085, 1077, 1089, 1095, 1077, 1090, 1085, 1082, 1086, 1075, 1086, 1088, 1090]\n"
          ]
        }
      ]
    }
  ]
}