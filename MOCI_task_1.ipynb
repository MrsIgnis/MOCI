{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdBeACrvkLauE2Y5BK22EN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrsIgnis/MOCI/blob/main/MOCI_task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I. Провести на любом тексте лемматизацию и стемминг (nltk, pymorphy2, pymorphy3, natasha)**"
      ],
      "metadata": {
        "id": "kNlsnX2jvOfG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "ILiUyuIzkzyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777a0627-4392-4c57-ebaa-c0e7fa38482c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pymorphy3\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "14hVZlhkuO9w"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zEssSou8QNE",
        "outputId": "48dcc1a4-67a1-442a-9655-5096501e1705"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_ru = set(stopwords.words(\"russian\"))\n",
        "stop_words_en = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "2gWBKQQvM5Yi"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/test.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "print(text)"
      ],
      "metadata": {
        "id": "4wDv_TmB4Q5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a499ff58-4ba3-4c35-c0dd-3c20832f5d0a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When a humble bard\n",
            "Graced a ride along\n",
            "With Geralt of Rivia\n",
            "Along came this song.\n",
            "From when the White Wolf fought\n",
            "A silver-tongued devil\n",
            "His army of elves...\n",
            "\n",
            "Когда скромняга бард отдыхал от дел\n",
            "С Геральтом из Ривии он песню эту пел.\n",
            "Сразился Белый Волк с велиречивым чертом\n",
            "Эльфов покромсал несчетные когорты.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer_ru = SnowballStemmer(\"russian\")\n",
        "stemmer_en = SnowballStemmer(\"english\")\n",
        "lemma_ru = pymorphy3.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "1qU-ix758f7V"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text: str) -> str:\n",
        "    words = word_tokenize(text)\n",
        "    lemmas = [lemma_ru.parse(word)[0].normal_form for word in words if word.isalpha() and word not in stop_words_ru]\n",
        "    return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "Z9Vtq-G7-Tyk"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_text(text: str) -> str:\n",
        "    words = word_tokenize(text)\n",
        "    stems = [stemmer_ru.stem(word) for word in words if word.isalpha() and word not in stop_words_ru]\n",
        "    return ' '.join(stems)"
      ],
      "metadata": {
        "id": "93W4PSArwET1"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas_text = lemmatize_text(text)\n",
        "stems_text = stem_text(text)\n",
        "print(\"Лемматизированный текст:\", lemmas_text, '\\n')\n",
        "print(\"Стеммированный текст:\", stems_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkYbkwHY32aa",
        "outputId": "b9590818-abff-4211-fea5-f932f1e7fe8a"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированный текст: when a humble bard graced a ride along with geralt of rivia along came this song from when the white wolf fought a devil his army of elves когда скромняга бард отдыхать дело с геральт ривия песня петь сразиться белый волк велиречивый чёрт эльф покромсать несчётный когорта \n",
            "\n",
            "Стеммированный текст: When a humble bard Graced a ride along With Geralt of Rivia Along came this song From when the White Wolf fought A devil His army of elves когд скромняг бард отдыха дел с геральт рив песн пел сраз бел волк велиречив черт эльф покромса несчетн когорт\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Написать функцию для токенизации всех символов из ASCII**"
      ],
      "metadata": {
        "id": "SqVLYWF0wrhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_ascii(text: str) -> list[str]:\n",
        "    return [char for char in text if not char.isspace()]"
      ],
      "metadata": {
        "id": "sB8h7F9nJfx3"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ascii_tokens = tokenize_ascii(text)\n",
        "print(\"Токенизированный текст:\", ascii_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUW0hhClw3-w",
        "outputId": "a14a2eab-7207-40a0-b0c4-9667197410ae"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизированный текст: ['W', 'h', 'e', 'n', 'a', 'h', 'u', 'm', 'b', 'l', 'e', 'b', 'a', 'r', 'd', 'G', 'r', 'a', 'c', 'e', 'd', 'a', 'r', 'i', 'd', 'e', 'a', 'l', 'o', 'n', 'g', 'W', 'i', 't', 'h', 'G', 'e', 'r', 'a', 'l', 't', 'o', 'f', 'R', 'i', 'v', 'i', 'a', 'A', 'l', 'o', 'n', 'g', 'c', 'a', 'm', 'e', 't', 'h', 'i', 's', 's', 'o', 'n', 'g', '.', 'F', 'r', 'o', 'm', 'w', 'h', 'e', 'n', 't', 'h', 'e', 'W', 'h', 'i', 't', 'e', 'W', 'o', 'l', 'f', 'f', 'o', 'u', 'g', 'h', 't', 'A', 's', 'i', 'l', 'v', 'e', 'r', '-', 't', 'o', 'n', 'g', 'u', 'e', 'd', 'd', 'e', 'v', 'i', 'l', 'H', 'i', 's', 'a', 'r', 'm', 'y', 'o', 'f', 'e', 'l', 'v', 'e', 's', '.', '.', '.', 'К', 'о', 'г', 'д', 'а', 'с', 'к', 'р', 'о', 'м', 'н', 'я', 'г', 'а', 'б', 'а', 'р', 'д', 'о', 'т', 'д', 'ы', 'х', 'а', 'л', 'о', 'т', 'д', 'е', 'л', 'С', 'Г', 'е', 'р', 'а', 'л', 'ь', 'т', 'о', 'м', 'и', 'з', 'Р', 'и', 'в', 'и', 'и', 'о', 'н', 'п', 'е', 'с', 'н', 'ю', 'э', 'т', 'у', 'п', 'е', 'л', '.', 'С', 'р', 'а', 'з', 'и', 'л', 'с', 'я', 'Б', 'е', 'л', 'ы', 'й', 'В', 'о', 'л', 'к', 'с', 'в', 'е', 'л', 'и', 'р', 'е', 'ч', 'и', 'в', 'ы', 'м', 'ч', 'е', 'р', 'т', 'о', 'м', 'Э', 'л', 'ь', 'ф', 'о', 'в', 'п', 'о', 'к', 'р', 'о', 'м', 'с', 'а', 'л', 'н', 'е', 'с', 'ч', 'е', 'т', 'н', 'ы', 'е', 'к', 'о', 'г', 'о', 'р', 'т', 'ы', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III. Написать функцию для векторизации всех символов из ASCII**"
      ],
      "metadata": {
        "id": "tXkTVNsI1RqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_ascii(text: list[str]) -> list[int]:\n",
        "    return [ord(char) for char in text if not char.isspace()]"
      ],
      "metadata": {
        "id": "SaK6Q569X_Eh"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ascii_vectors = vectorize_ascii(text)\n",
        "print(\"Векторизированный текст:\", ascii_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxG5ksm11YRg",
        "outputId": "8400152d-ddbb-4e23-c36f-e5060ea3d991"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Векторизированный текст: [87, 104, 101, 110, 97, 104, 117, 109, 98, 108, 101, 98, 97, 114, 100, 71, 114, 97, 99, 101, 100, 97, 114, 105, 100, 101, 97, 108, 111, 110, 103, 87, 105, 116, 104, 71, 101, 114, 97, 108, 116, 111, 102, 82, 105, 118, 105, 97, 65, 108, 111, 110, 103, 99, 97, 109, 101, 116, 104, 105, 115, 115, 111, 110, 103, 46, 70, 114, 111, 109, 119, 104, 101, 110, 116, 104, 101, 87, 104, 105, 116, 101, 87, 111, 108, 102, 102, 111, 117, 103, 104, 116, 65, 115, 105, 108, 118, 101, 114, 45, 116, 111, 110, 103, 117, 101, 100, 100, 101, 118, 105, 108, 72, 105, 115, 97, 114, 109, 121, 111, 102, 101, 108, 118, 101, 115, 46, 46, 46, 1050, 1086, 1075, 1076, 1072, 1089, 1082, 1088, 1086, 1084, 1085, 1103, 1075, 1072, 1073, 1072, 1088, 1076, 1086, 1090, 1076, 1099, 1093, 1072, 1083, 1086, 1090, 1076, 1077, 1083, 1057, 1043, 1077, 1088, 1072, 1083, 1100, 1090, 1086, 1084, 1080, 1079, 1056, 1080, 1074, 1080, 1080, 1086, 1085, 1087, 1077, 1089, 1085, 1102, 1101, 1090, 1091, 1087, 1077, 1083, 46, 1057, 1088, 1072, 1079, 1080, 1083, 1089, 1103, 1041, 1077, 1083, 1099, 1081, 1042, 1086, 1083, 1082, 1089, 1074, 1077, 1083, 1080, 1088, 1077, 1095, 1080, 1074, 1099, 1084, 1095, 1077, 1088, 1090, 1086, 1084, 1069, 1083, 1100, 1092, 1086, 1074, 1087, 1086, 1082, 1088, 1086, 1084, 1089, 1072, 1083, 1085, 1077, 1089, 1095, 1077, 1090, 1085, 1099, 1077, 1082, 1086, 1075, 1086, 1088, 1090, 1099, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IV. Провести токенизацию и векторизацию текста после лемматизации и стемминга**"
      ],
      "metadata": {
        "id": "sqNdsIbq1kqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas_ascii_tokens = tokenize_ascii(lemmas_text)\n",
        "lemmas_ascii_vectors = vectorize_ascii(lemmas_ascii_tokens)\n",
        "print(\"Токенизация лемм (ASCII):\", lemmas_ascii_tokens, '\\n')\n",
        "print(\"Векторизация лемм (ASCII):\", lemmas_ascii_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQoOaTww11Rs",
        "outputId": "0e265c14-2b92-4512-a173-bbebf3dda5b7"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация лемм (ASCII): ['w', 'h', 'e', 'n', 'a', 'h', 'u', 'm', 'b', 'l', 'e', 'b', 'a', 'r', 'd', 'g', 'r', 'a', 'c', 'e', 'd', 'a', 'r', 'i', 'd', 'e', 'a', 'l', 'o', 'n', 'g', 'w', 'i', 't', 'h', 'g', 'e', 'r', 'a', 'l', 't', 'o', 'f', 'r', 'i', 'v', 'i', 'a', 'a', 'l', 'o', 'n', 'g', 'c', 'a', 'm', 'e', 't', 'h', 'i', 's', 's', 'o', 'n', 'g', 'f', 'r', 'o', 'm', 'w', 'h', 'e', 'n', 't', 'h', 'e', 'w', 'h', 'i', 't', 'e', 'w', 'o', 'l', 'f', 'f', 'o', 'u', 'g', 'h', 't', 'a', 'd', 'e', 'v', 'i', 'l', 'h', 'i', 's', 'a', 'r', 'm', 'y', 'o', 'f', 'e', 'l', 'v', 'e', 's', 'к', 'о', 'г', 'д', 'а', 'с', 'к', 'р', 'о', 'м', 'н', 'я', 'г', 'а', 'б', 'а', 'р', 'д', 'о', 'т', 'д', 'ы', 'х', 'а', 'т', 'ь', 'д', 'е', 'л', 'о', 'с', 'г', 'е', 'р', 'а', 'л', 'ь', 'т', 'р', 'и', 'в', 'и', 'я', 'п', 'е', 'с', 'н', 'я', 'п', 'е', 'т', 'ь', 'с', 'р', 'а', 'з', 'и', 'т', 'ь', 'с', 'я', 'б', 'е', 'л', 'ы', 'й', 'в', 'о', 'л', 'к', 'в', 'е', 'л', 'и', 'р', 'е', 'ч', 'и', 'в', 'ы', 'й', 'ч', 'ё', 'р', 'т', 'э', 'л', 'ь', 'ф', 'п', 'о', 'к', 'р', 'о', 'м', 'с', 'а', 'т', 'ь', 'н', 'е', 'с', 'ч', 'ё', 'т', 'н', 'ы', 'й', 'к', 'о', 'г', 'о', 'р', 'т', 'а'] \n",
            "\n",
            "Векторизация лемм (ASCII): [119, 104, 101, 110, 97, 104, 117, 109, 98, 108, 101, 98, 97, 114, 100, 103, 114, 97, 99, 101, 100, 97, 114, 105, 100, 101, 97, 108, 111, 110, 103, 119, 105, 116, 104, 103, 101, 114, 97, 108, 116, 111, 102, 114, 105, 118, 105, 97, 97, 108, 111, 110, 103, 99, 97, 109, 101, 116, 104, 105, 115, 115, 111, 110, 103, 102, 114, 111, 109, 119, 104, 101, 110, 116, 104, 101, 119, 104, 105, 116, 101, 119, 111, 108, 102, 102, 111, 117, 103, 104, 116, 97, 100, 101, 118, 105, 108, 104, 105, 115, 97, 114, 109, 121, 111, 102, 101, 108, 118, 101, 115, 1082, 1086, 1075, 1076, 1072, 1089, 1082, 1088, 1086, 1084, 1085, 1103, 1075, 1072, 1073, 1072, 1088, 1076, 1086, 1090, 1076, 1099, 1093, 1072, 1090, 1100, 1076, 1077, 1083, 1086, 1089, 1075, 1077, 1088, 1072, 1083, 1100, 1090, 1088, 1080, 1074, 1080, 1103, 1087, 1077, 1089, 1085, 1103, 1087, 1077, 1090, 1100, 1089, 1088, 1072, 1079, 1080, 1090, 1100, 1089, 1103, 1073, 1077, 1083, 1099, 1081, 1074, 1086, 1083, 1082, 1074, 1077, 1083, 1080, 1088, 1077, 1095, 1080, 1074, 1099, 1081, 1095, 1105, 1088, 1090, 1101, 1083, 1100, 1092, 1087, 1086, 1082, 1088, 1086, 1084, 1089, 1072, 1090, 1100, 1085, 1077, 1089, 1095, 1105, 1090, 1085, 1099, 1081, 1082, 1086, 1075, 1086, 1088, 1090, 1072]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stems_ascii_tokens = tokenize_ascii(stems_text)\n",
        "stems_ascii_vectors = vectorize_ascii(stems_ascii_tokens)\n",
        "print(\"Токенизация стеммов (ASCII):\", stems_ascii_tokens, '\\n')\n",
        "print(\"Векторизация стеммов (ASCII):\", stems_ascii_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0EAYvau2IlU",
        "outputId": "bfc021d3-f1ae-4e3c-b288-e8dc44254d86"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация стеммов (ASCII): ['W', 'h', 'e', 'n', 'a', 'h', 'u', 'm', 'b', 'l', 'e', 'b', 'a', 'r', 'd', 'G', 'r', 'a', 'c', 'e', 'd', 'a', 'r', 'i', 'd', 'e', 'a', 'l', 'o', 'n', 'g', 'W', 'i', 't', 'h', 'G', 'e', 'r', 'a', 'l', 't', 'o', 'f', 'R', 'i', 'v', 'i', 'a', 'A', 'l', 'o', 'n', 'g', 'c', 'a', 'm', 'e', 't', 'h', 'i', 's', 's', 'o', 'n', 'g', 'F', 'r', 'o', 'm', 'w', 'h', 'e', 'n', 't', 'h', 'e', 'W', 'h', 'i', 't', 'e', 'W', 'o', 'l', 'f', 'f', 'o', 'u', 'g', 'h', 't', 'A', 'd', 'e', 'v', 'i', 'l', 'H', 'i', 's', 'a', 'r', 'm', 'y', 'o', 'f', 'e', 'l', 'v', 'e', 's', 'к', 'о', 'г', 'д', 'с', 'к', 'р', 'о', 'м', 'н', 'я', 'г', 'б', 'а', 'р', 'д', 'о', 'т', 'д', 'ы', 'х', 'а', 'д', 'е', 'л', 'с', 'г', 'е', 'р', 'а', 'л', 'ь', 'т', 'р', 'и', 'в', 'п', 'е', 'с', 'н', 'п', 'е', 'л', 'с', 'р', 'а', 'з', 'б', 'е', 'л', 'в', 'о', 'л', 'к', 'в', 'е', 'л', 'и', 'р', 'е', 'ч', 'и', 'в', 'ч', 'е', 'р', 'т', 'э', 'л', 'ь', 'ф', 'п', 'о', 'к', 'р', 'о', 'м', 'с', 'а', 'н', 'е', 'с', 'ч', 'е', 'т', 'н', 'к', 'о', 'г', 'о', 'р', 'т'] \n",
            "\n",
            "Векторизация стеммов (ASCII): [87, 104, 101, 110, 97, 104, 117, 109, 98, 108, 101, 98, 97, 114, 100, 71, 114, 97, 99, 101, 100, 97, 114, 105, 100, 101, 97, 108, 111, 110, 103, 87, 105, 116, 104, 71, 101, 114, 97, 108, 116, 111, 102, 82, 105, 118, 105, 97, 65, 108, 111, 110, 103, 99, 97, 109, 101, 116, 104, 105, 115, 115, 111, 110, 103, 70, 114, 111, 109, 119, 104, 101, 110, 116, 104, 101, 87, 104, 105, 116, 101, 87, 111, 108, 102, 102, 111, 117, 103, 104, 116, 65, 100, 101, 118, 105, 108, 72, 105, 115, 97, 114, 109, 121, 111, 102, 101, 108, 118, 101, 115, 1082, 1086, 1075, 1076, 1089, 1082, 1088, 1086, 1084, 1085, 1103, 1075, 1073, 1072, 1088, 1076, 1086, 1090, 1076, 1099, 1093, 1072, 1076, 1077, 1083, 1089, 1075, 1077, 1088, 1072, 1083, 1100, 1090, 1088, 1080, 1074, 1087, 1077, 1089, 1085, 1087, 1077, 1083, 1089, 1088, 1072, 1079, 1073, 1077, 1083, 1074, 1086, 1083, 1082, 1074, 1077, 1083, 1080, 1088, 1077, 1095, 1080, 1074, 1095, 1077, 1088, 1090, 1101, 1083, 1100, 1092, 1087, 1086, 1082, 1088, 1086, 1084, 1089, 1072, 1085, 1077, 1089, 1095, 1077, 1090, 1085, 1082, 1086, 1075, 1086, 1088, 1090]\n"
          ]
        }
      ]
    }
  ]
}